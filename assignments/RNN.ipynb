{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2jEVy63zrd4"
      },
      "source": [
        "## **Recurrent neural network scratch**\n",
        "\n",
        "---\n",
        "\n",
        "### **`[Problem 1] Simple Forward propagation implementation of RNN`**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "1NykcYN_zk4o"
      },
      "outputs": [],
      "source": [
        "#Library\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "W2UWhRAi7C9S"
      },
      "outputs": [],
      "source": [
        "class ScratchSimpleRNNClassifier:\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, x, w_x, w_h):\n",
        "        self.w_x = w_x\n",
        "        self.w_h = w_h\n",
        "        self.batch_size = x.shape[0] # 1\n",
        "        self.n_sequences = x.shape[1] # 3\n",
        "        self.n_features = x.shape[2] # 2\n",
        "        self.n_nodes = w_x.shape[1] # 4\n",
        "        self.h = np.zeros((self.batch_size, self.n_nodes)) # (batch_size, n_nodes)\n",
        "        self.b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
        "      \n",
        "        \n",
        "    def forward(self,x):\n",
        "      '''\n",
        "\n",
        "      '''\n",
        "      self.x = x\n",
        "      for n in range(self.n_sequences):\n",
        "          self.h = np.tanh(x[:, n, :] @ self.w_x + self.h @ self.w_h + self.b)\n",
        "      return self.h\n",
        "\n",
        "\n",
        "    def backward(self, dA):\n",
        "      \"\"\"\n",
        "      \"\"\"\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0bIfpde78Zy"
      },
      "source": [
        "### **`[Problem 2] Experiment of forward propagation with small sequence`**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQqHMlw57vfP",
        "outputId": "ed3ffb63-fe49-4ef4-9066-21760d29b6f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
        "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
        "\n",
        "rnn = ScratchSimpleRNNClassifier(x=x, w_h=w_h, w_x=w_x)\n",
        "\n",
        "rnn.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFmSMJVa8Zas"
      },
      "source": [
        "### **`[Problem 3] (Advance assignment) Implementation of backpropagation`**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "FFvuH31h8LHh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "class ScratchSimpleRNNClassifier:\n",
        "\n",
        "  def __init__(self, x, w_x, w_h):\n",
        "        self.w_x = w_x\n",
        "        self.w_h = w_h\n",
        "        self.batch_size = x.shape[0] # 1\n",
        "        self.n_sequences = x.shape[1] # 3\n",
        "        self.n_features = x.shape[2] # 2\n",
        "        self.n_nodes = w_x.shape[1] # 4\n",
        "        self.bh = np.zeros((self.batch_size, self.n_nodes)) # (batch_size, n_nodes)\n",
        "        self.by = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    '''\n",
        "    Perform a forward pass of the RNN using the given inputs.\n",
        "    Returns the final output and hidden state.\n",
        "    - inputs is an array of one hot vectors with shape (input_size, 1).\n",
        "    '''\n",
        "\n",
        "    self.last_inputs = inputs\n",
        "    self.last_hs = { 0: self.bh }\n",
        "\n",
        "    # Perform each step of the RNN\n",
        "    for i in enumerate(inputs):\n",
        "      self.bh = np.tanh(inputs[:, i, :] @ self.w_x + self.bh @ self.w_h + self.by)\n",
        "      self.last_hs[i + 1] = self.bh\n",
        "\n",
        "\n",
        "    return self.bh\n",
        "\n",
        "  def backprop(self, d_y, learn_rate=2e-2):\n",
        "    '''\n",
        "    Perform a backward pass of the RNN.\n",
        "    - d_y (dL/dy) has shape (output_size, 1).\n",
        "    - learn_rate is a float.\n",
        "    '''\n",
        "    n = len(self.last_inputs)\n",
        "\n",
        "    # Calculate dL/dWhy and dL/dby.\n",
        "    d_W_x = d_y @ self.last_hs[n].T\n",
        "    d_by = d_y\n",
        "\n",
        "    # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
        "    d_W_h = np.zeros(self.w_h.shape)\n",
        "    d_W_x = np.zeros(self.w_x.shape)\n",
        "    d_bh = np.zeros(self.bh.shape)\n",
        "\n",
        "    # Calculate dL/dh for the last h.\n",
        "    # dL/dh = dL/dy * dy/dh\n",
        "    d_h =  d_y\n",
        "\n",
        "    # Backpropagate through time.\n",
        "    for t in reversed(range(n)):\n",
        "      # An intermediate value: dL/dh * (1 - h^2)\n",
        "      temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
        "\n",
        "      # dL/db = dL/dh * (1 - h^2)\n",
        "      d_bh += temp\n",
        "\n",
        "      # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
        "      d_Whh += temp @ self.last_hs[t].T\n",
        "\n",
        "      # dL/dWxh = dL/dh * (1 - h^2) * x\n",
        "      d_Wxh += temp @ self.last_inputs[t].T\n",
        "\n",
        "      # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
        "      d_h = self.Whh @ temp\n",
        "\n",
        "    # Clip to prevent exploding gradients.\n",
        "    for d in [d_Wxh, d_Whh, d_bh, d_by]:\n",
        "      np.clip(d, -1, 1, out=d)\n",
        "\n",
        "    # Update weights and biases using gradient descent.\n",
        "    self.w_h -= learn_rate * d_Whh\n",
        "    self.w_x -= learn_rate * d_Wxh\n",
        "    self.bh -= learn_rate * d_bh\n",
        "    self.by -= learn_rate * d_by"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxm66Gqb2CVeSYD+vkUxA3",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
